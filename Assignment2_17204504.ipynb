{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping and pre-processing a corpus of news article from a set of web-pages and evaluating the performance of automated classification of these articles in a supervised learning context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import scipy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the monthly news URL's from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 12 months news lists\n"
     ]
    }
   ],
   "source": [
    "r  = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/index.html\")\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data, \"lxml\")\n",
    "monthly_news = []\n",
    "\n",
    "lists = soup.find_all('li')\n",
    "    \n",
    "for link in lists:\n",
    "    monthly_news.append(link.find(\"a\").get('href'))\n",
    "\n",
    "print(\"Scraped %d months news lists\" % len(monthly_news) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving news urls and category lables for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1394 news corpus\n"
     ]
    }
   ],
   "source": [
    "articles = {} # Creating a dictionary to store articles and category\n",
    "\n",
    "for month in monthly_news:\n",
    "    \n",
    "    r  = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/\" + month)\n",
    "\n",
    "    data = r.text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    news_list = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    \n",
    "    for news in news_list:\n",
    "        category_td = news.find(\"td\", {\"class\": \"category\"})\n",
    "        category = category_td.text.strip() #Removing the escape characters from category\n",
    "                \n",
    "        if (category != 'N/A'):\n",
    "            news_links = news.find(\"td\", {\"class\": \"title\"})\n",
    "            news_href = news_links.find(\"a\").get('href')\n",
    "            \n",
    "            article_link = \"http://mlg.ucd.ie/modules/COMP41680/archive/\" + news_href\n",
    "            \n",
    "            r  = requests.get(article_link)\n",
    "\n",
    "            data = r.text\n",
    "\n",
    "            soup = BeautifulSoup(data, \"lxml\") \n",
    "            \n",
    "            article_text = ''\n",
    "            article = soup.find(\"div\", {\"class\":\"main\"}).findAll('p')\n",
    "            for element in article:\n",
    "                article_text += ' '.join(element.findAll(text = True))\n",
    "                \n",
    "            \n",
    "            article_text = article_text.replace('\"', '\\\\\"')\n",
    "            articles[article_text] = category  \n",
    "\n",
    "print(\"Scraped %d news corpus\" % len(articles) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Storing the articles to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = \"articles.csv\"\n",
    "fo = open(fout, \"w\")\n",
    "\n",
    "fo.write('Article|Category\\n')\n",
    "for k, v in articles.items():\n",
    "    fo.write(str(k) + '|'+ str(v) + '\\n')\n",
    "\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sporting industry has come a long way sinc...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian quake hits European sharesShares in Euro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BT is offering customers free internet telepho...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barclays shares up on merger talkShares in UK ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>England centre Olly Barkley has been passed fi...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article    Category\n",
       "0  The sporting industry has come a long way sinc...  technology\n",
       "1  Asian quake hits European sharesShares in Euro...    business\n",
       "2  BT is offering customers free internet telepho...  technology\n",
       "3  Barclays shares up on merger talkShares in UK ...    business\n",
       "4  England centre Olly Barkley has been passed fi...       sport"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"articles.csv\", delimiter=\"|\",engine='python')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n"
     ]
    }
   ],
   "source": [
    "articles_list = raw_data['Article'].tolist() #Getting news corpus to data\n",
    "print(len(articles_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing and Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the corpuses we will be following these steps:\n",
    "1. Splitting raw text to tokens (Tokenization)\n",
    "2. Converting all text to lower case\n",
    "3. Removing short terms and stop words\n",
    "4. Stem/Lemmatise tokens\n",
    "5. Filter out infrequent terms\n",
    "6. Giving weights to terms\n",
    "7. Creating a document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Scikit-learn converts tokens to lowercase and removes\n",
    "tokens of length 1 (i.e. single letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6703)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",  min_df = 5) # Preprocessing data to remove stop words and filter out terms appearing in less than 5 documents \n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "\n",
    "print(filtered_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also build a vocabulary for the corpus, both in the form of a list and in the form of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 6703 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appointed', 'appointment', 'appreciate', 'approach', 'approached', 'appropriate', 'approval', 'approvals', 'approve', 'approved', 'april', 'arabia', 'arbitration', 'arcade', 'arch', 'architecture', 'arcy', 'area', 'areas', 'aren', 'arena', 'argentina', 'argentine', 'arguably', 'argue', 'argued', 'argues', 'arguing', 'argument', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "print(terms[500:530])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use NLTK lemmatisation with Scikit-learn, we need to create a custom tokenisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our custom tokenizer with the standard CountVectorizer approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6010)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sporting', 'industry', 'ha', 'come', 'long', 'way', '60', 'niche', 'root', 'deep', 'sport', 'showing', 'sign', 'decline', 'time', 'soon', 'later', 'reason', 'seemingly', 'difference', 'customer', 'fan', 'leader', 'ownership', 'group', 'king', 'explained', 'face', 'ceo', 'business', 'dying', 'position', 'passion', 'certainly', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the usefulness of the document-term matrix by giving higher weights to more \"important\" terms.\n",
    "The most common normalisation is term frequencyâ€“inverse document frequency (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using TfidfVectorizer() in place of CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6010)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "# display some sample weighted values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same Scikit-learn functionality to create a document-term matrix with N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 9600)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters and also We specify an extra parameter ngram_range which specifies the shortest and longest token sequences to include.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer,ngram_range = (1,2))\n",
    "document_matrix = vectorizer.fit_transform(articles_list)\n",
    "# display some sample weighted values\n",
    "print(document_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this approach significantly increases the size of the vocabulary for a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create classifiers we need to divide the dataset into training and test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = raw_data['Article'].tolist() #Articles\n",
    "Y = raw_data['Category'].tolist() # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burhanuddinshakir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous vectorizer to create a document-matrix of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance details\n",
    "# accuracy = metrics.accuracy_score(y_train, predicted) # , normalize=True, sample_weight=None\n",
    "# print(\"Accuracy: \" +  str(accuracy))\n",
    "# print(metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
