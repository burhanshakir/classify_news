{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping and pre-processing a corpus of news article from a set of web-pages and evaluating the performance of automated classification of these articles in a supervised learning context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import scipy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the monthly news URL's from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 12 months news lists\n"
     ]
    }
   ],
   "source": [
    "r  = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/index.html\")\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data, \"lxml\")\n",
    "monthly_news = []\n",
    "\n",
    "lists = soup.find_all('li')\n",
    "    \n",
    "for link in lists:\n",
    "    monthly_news.append(link.find(\"a\").get('href'))\n",
    "\n",
    "print(\"Scraped %d months news lists\" % len(monthly_news) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving news urls and category lables for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1394 news corpus\n"
     ]
    }
   ],
   "source": [
    "articles = {} # Creating a dictionary to store articles and category\n",
    "\n",
    "for month in monthly_news:\n",
    "    \n",
    "    r  = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/\" + month)\n",
    "\n",
    "    data = r.text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    news_list = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    \n",
    "    for news in news_list:\n",
    "        category_td = news.find(\"td\", {\"class\": \"category\"})\n",
    "        category = category_td.text.strip() #Removing the escape characters from category\n",
    "                \n",
    "        if (category != 'N/A'):\n",
    "            news_links = news.find(\"td\", {\"class\": \"title\"})\n",
    "            news_href = news_links.find(\"a\").get('href')\n",
    "            \n",
    "            article_link = \"http://mlg.ucd.ie/modules/COMP41680/archive/\" + news_href\n",
    "            \n",
    "            r  = requests.get(article_link)\n",
    "\n",
    "            data = r.text\n",
    "\n",
    "            soup = BeautifulSoup(data, \"lxml\") \n",
    "            \n",
    "            article_text = ''\n",
    "            article = soup.find(\"div\", {\"class\":\"main\"}).findAll('p')\n",
    "            for element in article:\n",
    "                article_text += ' '.join(element.findAll(text = True))\n",
    "                \n",
    "            \n",
    "            article_text = article_text.replace('\"', '\\\\\"')\n",
    "            articles[article_text] = category  \n",
    "\n",
    "print(\"Scraped %d news corpus\" % len(articles) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Storing the articles to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fout = \"articles.csv\"\n",
    "fo = open(fout, \"w\")\n",
    "\n",
    "fo.write('Article|Category\\n')\n",
    "for k, v in articles.items():\n",
    "    fo.write(str(k) + '|'+ str(v) + '\\n')\n",
    "\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sporting industry has come a long way sinc...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian quake hits European sharesShares in Euro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BT is offering customers free internet telepho...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barclays shares up on merger talkShares in UK ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>England centre Olly Barkley has been passed fi...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article    Category\n",
       "0  The sporting industry has come a long way sinc...  technology\n",
       "1  Asian quake hits European sharesShares in Euro...    business\n",
       "2  BT is offering customers free internet telepho...  technology\n",
       "3  Barclays shares up on merger talkShares in UK ...    business\n",
       "4  England centre Olly Barkley has been passed fi...       sport"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"articles.csv\", delimiter=\"|\",engine='python')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n"
     ]
    }
   ],
   "source": [
    "articles_list = raw_data['Article'].tolist() #Getting news corpus to data\n",
    "print(len(articles_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing and Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the corpuses we will be following these steps:\n",
    "1. Splitting raw text to tokens (Tokenization)\n",
    "2. Converting all text to lower case\n",
    "3. Removing short terms and stop words\n",
    "4. Stem/Lemmatise tokens\n",
    "5. Filter out infrequent terms\n",
    "6. Giving weights to terms\n",
    "7. Creating a document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Scikit-learn converts tokens to lowercase and removes\n",
    "tokens of length 1 (i.e. single letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6703)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",  min_df = 5) # Preprocessing data to remove stop words and filter out terms appearing in less than 5 documents \n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also build a vocabulary for the corpus, both in the form of a list and in the form of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 6703 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appointed', 'appointment', 'appreciate', 'approach', 'approached', 'appropriate', 'approval', 'approvals', 'approve', 'approved', 'april', 'arabia', 'arbitration', 'arcade', 'arch', 'architecture', 'arcy', 'area', 'areas', 'aren', 'arena', 'argentina', 'argentine', 'arguably', 'argue', 'argued', 'argues', 'arguing', 'argument', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "print(terms[500:530])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use NLTK lemmatisation with Scikit-learn, we need to create a custom tokenisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our custom tokenizer with the standard CountVectorizer approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6010)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sporting', 'industry', 'ha', 'come', 'long', 'way', '60', 'niche', 'root', 'deep', 'sport', 'showing', 'sign', 'decline', 'time', 'soon', 'later', 'reason', 'seemingly', 'difference', 'customer', 'fan', 'leader', 'ownership', 'group', 'king', 'explained', 'face', 'ceo', 'business', 'dying', 'position', 'passion', 'certainly', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the usefulness of the document-term matrix by giving higher weights to more \"important\" terms.\n",
    "The most common normalisation is term frequencyâ€“inverse document frequency (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using TfidfVectorizer() in place of CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 6010)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(articles_list)\n",
    "# display some sample weighted values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same Scikit-learn functionality to create a document-term matrix with N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1394, 9600)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters and also We specify an extra parameter ngram_range which specifies the shortest and longest token sequences to include.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5,tokenizer=lemma_tokenizer,ngram_range = (1,2))\n",
    "document_matrix = vectorizer.fit_transform(articles_list)\n",
    "# display some sample weighted values\n",
    "print(document_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this approach significantly increases the size of the vocabulary for a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create classifiers we need to divide the dataset into training and test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = raw_data['Article'].tolist() #Articles\n",
    "Y = raw_data['Category'].tolist() # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burhanuddinshakir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous vectorizer to create a document-matrix of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 5060)\n"
     ]
    }
   ],
   "source": [
    "train_X = vectorizer.fit_transform(X_train)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a KNN model for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a document-term matrix from test documents. We call transform() to use the same vocabulary as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(419, 5060)\n"
     ]
    }
   ],
   "source": [
    "test_X = vectorizer.transform(X_test)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions for our test documents using the KNN model and the test document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['technology', 'sport', 'sport', 'sport', 'sport'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model_knn.predict(test_X)\n",
    "predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.957040572792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   business       0.93      0.96      0.95       132\n",
      "      sport       0.99      0.95      0.97       171\n",
      " technology       0.93      0.97      0.95       116\n",
      "\n",
      "avg / total       0.96      0.96      0.96       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print performance details\n",
    "accuracy_knn = metrics.accuracy_score(y_test, predicted) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy_knn))\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Building a Naive Bayes model for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NB = MultinomialNB()\n",
    "model_NB.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions for our test documents using the Naive Bayes model and the test document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['technology', 'sport', 'sport', 'sport', 'sport'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model_NB.predict(test_X)\n",
    "predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.973747016706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   business       0.95      0.98      0.97       132\n",
      "      sport       0.99      0.98      0.99       171\n",
      " technology       0.98      0.95      0.96       116\n",
      "\n",
      "avg / total       0.97      0.97      0.97       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print performance details\n",
    "accuracy_nb = metrics.accuracy_score(y_test, predicted) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy_nb))\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we get is ~97.38%, which is not bad for a naive classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate each classifier using the K-Fold cross validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "#Prepare models\n",
    "models = []\n",
    "models.append(('KNN' , model_knn))\n",
    "models.append(('NB' , model_NB))\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "articles_X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running cross validation procedure using 10 -fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.959815 (0.009782)\n",
      "NB: 0.981321 (0.014824)\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, articles_X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ce36438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it would suggest that NB analysis provides better accuracy than KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most textual data arrives in an unstructured form without any predefined organisation or format, beyond natural language. The vocabulary, formatting, and quality of the text can vary significantly. Tokenising texts, removing stop words and infrequent words, Lemmatising and giving weightage to terms help us understand better what the text is trying to convey this in turn help us in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next big challenge arrives to transform tokens to numeric values so that Machine Learning models could train and classify unknown values. This is done using the Bag-of-words model, each document is represented by a vector in\n",
    "a m-dimensional coordinate space, where m is total number of unique terms across all documents (the corpus vocabulary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used NaiveBayes and K-Neireast Neighbours to classify our news corpus. Both have the algotihms gave us a good performance but Naive Bayes had a slightly better result. This can be because it assumes all terms are independent and classify them based on frequency counts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
